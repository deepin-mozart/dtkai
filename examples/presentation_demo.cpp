// SPDX-FileCopyrightText: 2025 UnionTech Software Technology Co., Ltd.
//
// SPDX-License-Identifier: LGPL-3.0-or-later

/**
 * @file presentation_demo.cpp
 * @brief Comprehensive AI Presentation Demo for Deepin System Assistant
 *
 * This demo demonstrates the integration of Speech-to-Text, Text-to-Speech,
 * NLP Chat, and Image Recognition capabilities in a sequential presentation format.
 *
 * Presentation Steps:
 * 1. Welcome and introduction (TTS)
 * 2. NLP demonstration - Deepin desktop background setting (TTS + Chat)
 * 3. Speech recognition demonstration (STT)
 * 4. Vision demonstration - Image analysis (TTS + Vision)
 * 5. Summary of demonstration (TTS)
 */

#include <DSpeechToText>
#include <DTextToSpeech>
#include <DChatCompletions>
#include <DImageRecognition>

#include <QCoreApplication>
#include <QTimer>
#include <QAudioInput>
#include <QAudioFormat>
#include <QAudioDeviceInfo>
#include <QIODevice>
#include <QByteArray>
#include <QJsonDocument>
#include <QJsonObject>
#include <QScopedPointer>
#include <QMutex>
#include <QThread>
#include <QEventLoop>
#include <QDateTime>
#include <QAudioOutput>
#include <QBuffer>
#include <QQueue>
#include <QMutexLocker>
#include <QDebug>
#include <QFile>
#include <QTextStream>
#include <QDir>
#include <functional>

DCORE_USE_NAMESPACE
DAI_USE_NAMESPACE

// Task definition for the presentation queue
struct PresentationTask {
    QString taskId;
    QString description;
    std::function<void()> executeFunction;
    std::function<bool()> completionChecker;
    
    PresentationTask(const QString &id, const QString &desc, 
                    std::function<void()> exec, std::function<bool()> checker)
        : taskId(id), description(desc), executeFunction(exec), completionChecker(checker) {}
};

class PresentationDemo : public QObject
{
    Q_OBJECT

public:
    PresentationDemo(QObject *parent = nullptr);
    ~PresentationDemo();

    void startPresentation();

private Q_SLOTS:
    // Speech-to-Text slots
    void onRecognitionResult(const QString &text);
    void onRecognitionPartialResult(const QString &partialText);
    void onRecognitionError(int errorCode, const QString &errorMessage);
    void onRecognitionCompleted(const QString &finalText);

    // Chat slots
    void onStreamOutput(const QString &content);
    void onStreamFinished(int error);

    // Text-to-Speech slots
    void onSynthesisResult(const QByteArray &audioData);
    void onSynthesisError(int errorCode, const QString &errorMessage);
    void onSynthesisCompleted(const QByteArray &finalAudio);

    // Audio playback slots
    void onAudioStateChanged(QAudio::State state);

    // Audio input slots
    void onAudioDataReady();

private:
    // Task queue management methods
    void initializeTaskQueue();
    void executeNextTask();
    void checkTaskCompletion();
    void completeCurrentTask();
    
    // Presentation flow methods
    void speakText(const QString &text);
    void waitForSpeechComplete();

    // Individual step implementations
    void step1_Welcome();
    void step2_NLPDemo();
    void step3_SpeechDemo();
    void step4_VisionDemo();
    void step5_Summary();

    // Utility methods
    void setupAudioInput();
    void setupAudioOutput();
    void startRecording();
    void stopRecording();
    void playAudioFile(const QString &filename);
    void saveAudioToFile(const QByteArray &audioData, const QString &filename);
    void displayStatus(const QString &module, const QString &status, const QString &details = "");
    void displayResult(const QString &module, const QString &result);

private:
    // AI services
    DSpeechToText *stt = nullptr;
    DChatCompletions *chat = nullptr;
    DTextToSpeech *tts = nullptr;
    DImageRecognition *imageRecognition = nullptr;

    // Audio input
    QAudioInput *audioInput = nullptr;
    QIODevice *audioDevice = nullptr;
    QAudioFormat audioFormat;
    bool isRecording = false;
    QString currentUserInput;

    // Audio output
    QAudioOutput *audioOutput = nullptr;
    QBuffer *audioBuffer = nullptr;
    QByteArray accumulatedAudioData; // Buffer for streaming audio

    // Task queue system
    QQueue<PresentationTask> taskQueue;
    PresentationTask *currentTask = nullptr;
    bool isTaskRunning = false;
    QTimer *taskCompletionTimer = nullptr;
    
    // Presentation state
    bool isPresentationRunning = false;
    bool isWaitingForSpeech = false;
    QString currentSpeechText;
    QString outputFilename;

    // Task completion tracking
    bool speechCompleted = false;
    bool chatCompleted = false;
    bool recognitionCompleted = false;
    bool visionCompleted = false;
    bool isAudioPlaying = false;
    bool isSynthesisCompleted = false;  // æ ‡è®°è¯­éŸ³åˆæˆæ˜¯å¦å®Œæˆ

    // Timers
    QTimer *stepTimer = nullptr;
    QTimer *recognitionTimeoutTimer = nullptr;
    QTimer *idleConfirmationTimer = nullptr;  // ç”¨äºç¡®è®¤éŸ³é¢‘æ’­æ”¾çœŸæ­£å®Œæˆçš„å®šæ—¶å™¨
    static const int RECOGNITION_TIMEOUT_MS = 5000; // 15 seconds timeout
    static const int IDLE_CONFIRMATION_MS = 200; // 2 seconds to confirm idle state
    static const int STEP_DELAY_MS = 200; // 2 seconds delay between steps
};

PresentationDemo::PresentationDemo(QObject *parent)
    : QObject(parent)
{
    qInfo() << "ğŸ¤– Creating Deepin AI Presentation Demo...";

    // Create AI service instances
    stt = new DSpeechToText(this);
    chat = new DChatCompletions(this);
    tts = new DTextToSpeech(this);
    imageRecognition = new DImageRecognition(this);

    // Connect Speech-to-Text signals
    connect(stt, &DSpeechToText::recognitionResult,
            this, &PresentationDemo::onRecognitionResult);
    connect(stt, &DSpeechToText::recognitionPartialResult,
            this, &PresentationDemo::onRecognitionPartialResult);
    connect(stt, &DSpeechToText::recognitionError,
            this, &PresentationDemo::onRecognitionError);
    connect(stt, &DSpeechToText::recognitionCompleted,
            this, &PresentationDemo::onRecognitionCompleted);

    // Connect Chat signals
    connect(chat, &DChatCompletions::streamOutput,
            this, &PresentationDemo::onStreamOutput);
    connect(chat, &DChatCompletions::streamFinished,
            this, &PresentationDemo::onStreamFinished);

    // Connect Text-to-Speech signals
    connect(tts, &DTextToSpeech::synthesisResult,
            this, &PresentationDemo::onSynthesisResult);
    connect(tts, &DTextToSpeech::synthesisError,
            this, &PresentationDemo::onSynthesisError);
    connect(tts, &DTextToSpeech::synthesisCompleted,
            this, &PresentationDemo::onSynthesisCompleted);

    // Setup audio format (16kHz, 16-bit, mono)
    audioFormat.setSampleRate(16000);
    audioFormat.setChannelCount(1);
    audioFormat.setSampleSize(16);
    audioFormat.setCodec("audio/pcm");
    audioFormat.setByteOrder(QAudioFormat::LittleEndian);
    audioFormat.setSampleType(QAudioFormat::SignedInt);

    // Create timers
    taskCompletionTimer = new QTimer(this);
    taskCompletionTimer->setSingleShot(false);
    taskCompletionTimer->setInterval(1000); // Check every second
    connect(taskCompletionTimer, &QTimer::timeout, this, &PresentationDemo::checkTaskCompletion);

    recognitionTimeoutTimer = new QTimer(this);
    recognitionTimeoutTimer->setSingleShot(true);
    connect(recognitionTimeoutTimer, &QTimer::timeout, [this]() {
        if (isRecording) {
            displayStatus("ğŸ“ Speech-to-Text", "â° Timeout", "Using latest partial result");
            if (!currentUserInput.isEmpty()) {
                displayResult("ğŸ“ Speech-to-Text", currentUserInput);
            }
            stopRecording();
            recognitionCompleted = true;
        }
    });

    idleConfirmationTimer = new QTimer(this);
    idleConfirmationTimer->setSingleShot(true);
    connect(idleConfirmationTimer, &QTimer::timeout, [this]() {
        // å»¶è¿Ÿç¡®è®¤ï¼šå¦‚æœè¯­éŸ³åˆæˆå·²å®Œæˆä¸”ä»ç„¶æ²¡æœ‰éŸ³é¢‘æ’­æ”¾ï¼Œåˆ™è®¤ä¸ºçœŸæ­£å®Œæˆ
        if (isSynthesisCompleted && !isAudioPlaying) {
            qInfo() << "ğŸ¤ Idle confirmation: Speech synthesis and playback both completed";
            speechCompleted = true;
            isWaitingForSpeech = false;
        } else {
            qInfo() << "ğŸ¤ Idle confirmation: Still waiting - synthesis:" << isSynthesisCompleted 
                    << ", audioPlaying:" << isAudioPlaying;
        }
    });

    qInfo() << "âœ… Deepin AI Presentation Demo created successfully";
}

PresentationDemo::~PresentationDemo()
{
    if (audioInput) {
        audioInput->stop();
    }
    if (audioOutput) {
        audioOutput->stop();
    }

    qInfo() << "âœ… Presentation Demo destroyed";
}

void PresentationDemo::startPresentation()
{
    qInfo() << "\n" << QString(80, '=');
    qInfo() << "ğŸ¤ Deepin AI Assistant - Comprehensive Presentation Demo";
    qInfo() << QString(80, '=');
    qInfo() << "ğŸ  Role: Deepin System Computer Manager Assistant";
    qInfo() << "ğŸ“‹ Presentation Steps:";
    qInfo() << "   1. ğŸ‘‹ Welcome and Introduction";
    qInfo() << "   2. ğŸ¤– NLP Demo - Deepin Desktop Background Setting";
    qInfo() << "   3. ğŸ¤ Speech Recognition Demo";
    qInfo() << "   4. ğŸ‘ï¸  Vision Demo - Image Analysis";
    qInfo() << "   5. ğŸ“ Presentation Summary";
    qInfo() << QString(80, '=');
    qInfo() << "ğŸ’¡ Each step will complete before moving to the next...";
    qInfo() << "â¹ï¸  Press Ctrl+C to stop the demo";
    qInfo() << QString(80, '=') << "\n";

    setupAudioInput();
    setupAudioOutput();
    
    // Initialize task queue
    initializeTaskQueue();

    isPresentationRunning = true;
    
    // Start executing tasks
    executeNextTask();
}

void PresentationDemo::initializeTaskQueue()
{
    qInfo() << "ğŸ”§ Initializing task queue...";

    // Task 1: Welcome and Introduction
    taskQueue.enqueue(PresentationTask(
        "welcome",
        "Welcome and Introduction",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: Welcome and Introduction";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            step1_Welcome();
        },
        [this]() { 
            // ç¡®ä¿è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));
    
    // Task 2: NLP Demo Introduction
    taskQueue.enqueue(PresentationTask(
        "nlp_intro",
        "NLP Demo Introduction",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: NLP Demo Introduction";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            QString introText = "æ¥ä¸‹æ¥æˆ‘å°†æ¼”ç¤ºè‡ªç„¶è¯­è¨€å¤„ç†åŠŸèƒ½ã€‚"
                               "æˆ‘å¯ä»¥å›ç­”å…³äºdeepinç³»ç»Ÿçš„å„ç§é—®é¢˜ã€‚"
                               "ç°åœ¨è®©æˆ‘æ¥å›ç­”ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼šå¦‚ä½•è®¾ç½®æ¡Œé¢èƒŒæ™¯ã€‚";
            displayStatus("ğŸ¤– Step 2", "ğŸ”Š Speaking", "NLP Demo Introduction");
            speakText(introText);
        },
        [this]() { 
            // ç¡®ä¿è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));
    
    // Task 3: NLP Processing and Response
    taskQueue.enqueue(PresentationTask(
        "nlp_processing",
        "NLP Processing and Response",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: NLP Processing and Response";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            chatCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            
            QString question = "deepinç³»ç»Ÿå¦‚ä½•è®¾ç½®æ¡Œé¢èƒŒæ™¯ï¼Ÿ";
            displayStatus("ğŸ¤– NLP Processing", "ğŸ”„ Processing", question);

            currentSpeechText = "è¦è®¾ç½®deepinç³»ç»Ÿçš„æ¡Œé¢èƒŒæ™¯ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n"
                               "ç¬¬ä¸€æ­¥ï¼Œå³é”®ç‚¹å‡»æ¡Œé¢ç©ºç™½åŒºåŸŸï¼Œé€‰æ‹©\"ä¸ªæ€§åŒ–\"é€‰é¡¹ã€‚\n"
                               "ç¬¬äºŒæ­¥ï¼Œåœ¨æ‰“å¼€çš„çª—å£ä¸­ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç³»ç»Ÿé¢„è®¾çš„å£çº¸ã€‚"
                               "è¿™æ ·å°±æˆåŠŸè®¾ç½®äº†æ‚¨çš„æ¡Œé¢èƒŒæ™¯ã€‚";

            QTimer::singleShot(200, [this]() {
                displayResult("ğŸ¤– NLP Response", currentSpeechText);
                chatCompleted = true;
                speakText(currentSpeechText);
            });
        },
        [this]() { 
            // ç¡®ä¿NLPå¤„ç†å®Œæˆã€è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && chatCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));
    
    // Task 4: Speech Recognition Demo Introduction
    taskQueue.enqueue(PresentationTask(
        "speech_intro",
        "Speech Recognition Demo Introduction",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: Speech Recognition Demo Introduction";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            QString introText = "ç°åœ¨æˆ‘å°†æ¼”ç¤ºè¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚"
                               "è¯·æ‚¨å¯¹ç€éº¦å…‹é£è¯´ä¸€æ®µè¯ï¼Œ"
                               "æˆ‘å°†æŠŠæ‚¨çš„è¯­éŸ³è½¬æ¢æˆæ–‡å­—æ˜¾ç¤ºå‡ºæ¥ã€‚"
                               "æˆ‘ç°åœ¨å¼€å§‹ç›‘å¬æ‚¨çš„è¯­éŸ³è¾“å…¥ã€‚";
            displayStatus("ğŸ¤ Step 3", "ğŸ”Š Speaking", "Speech Recognition Demo");
            speakText(introText);
        },
        [this]() { 
            // ç¡®ä¿è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));

    // Task 5: Speech Recognition Processing
    taskQueue.enqueue(PresentationTask(
        "speech_recognition",
        "Speech Recognition Processing",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: Speech Recognition Processing";
            qInfo() << QString(40, '-');
            recognitionCompleted = false;
            startRecording();
        },
        [this]() { return recognitionCompleted; }
    ));
    
    // Task 6: Vision Demo Introduction
    taskQueue.enqueue(PresentationTask(
        "vision_intro",
        "Vision Demo Introduction",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: Vision Demo Introduction";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            QString introText = "æ¥ä¸‹æ¥æˆ‘å°†æ¼”ç¤ºå›¾åƒè¯†åˆ«åŠŸèƒ½ã€‚"
                               "æˆ‘å°†åˆ†ææŒ‡å®šçš„å›¾ç‰‡ï¼Œ"
                               "ç„¶åå‘Šè¯‰æ‚¨å›¾ç‰‡ä¸­åŒ…å«çš„å†…å®¹ã€‚"
                               "è®©æˆ‘æ¥çœ‹çœ‹è¿™å¼ å›¾ç‰‡ã€‚";
            displayStatus("ğŸ‘ï¸ Step 4", "ğŸ”Š Speaking", "Vision Demo Introduction");
            speakText(introText);
        },
        [this]() { 
            // ç¡®ä¿è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));
    
    // Task 7: Vision Processing
    taskQueue.enqueue(PresentationTask(
        "vision_processing",
        "Vision Processing",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: Vision Processing";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            visionCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            
            QString imagePath = "/home/ut000824@uos/Desktop/temp/images.jpeg";
            QString prompt = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬çœ‹åˆ°äº†ä»€ä¹ˆç‰©ä½“ã€åœºæ™¯ã€äººç‰©æˆ–æ–‡å­—ç­‰ã€‚";
            
            displayStatus("ğŸ‘ï¸ Vision Processing", "ğŸ”„ Analyzing", imagePath);
            
            QString result = imageRecognition->recognizeImage(imagePath, prompt);
            auto error = imageRecognition->lastError();
            
            if (error.getErrorCode() != -1) {
                qWarning() << "å›¾åƒè¯†åˆ«å¤±è´¥:" << error.getErrorCode() << error.getErrorMessage();
                currentSpeechText = "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•åˆ†æè¿™å¼ å›¾ç‰‡ã€‚"
                                   "å¯èƒ½æ˜¯å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨æˆ–è€…å›¾åƒè¯†åˆ«æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
                                   "åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘å¯ä»¥è¯†åˆ«å›¾ç‰‡ä¸­çš„ç‰©ä½“ã€åœºæ™¯ã€æ–‡å­—ç­‰å†…å®¹ï¼Œ"
                                   "å¹¶ä¸ºæ‚¨æä¾›è¯¦ç»†çš„æè¿°ã€‚";
            } else if (result.isEmpty()) {
                currentSpeechText = "å›¾ç‰‡åˆ†æå®Œæˆï¼Œä½†æ˜¯æ²¡æœ‰è·å¾—æœ‰æ•ˆçš„è¯†åˆ«ç»“æœã€‚"
                                   "è¿™å¯èƒ½æ˜¯å› ä¸ºå›¾ç‰‡å†…å®¹æ¯”è¾ƒå¤æ‚æˆ–è€…å…‰çº¿æ¡ä»¶ä¸ä½³ã€‚";
            } else {
                currentSpeechText = QString("é€šè¿‡å›¾åƒè¯†åˆ«åˆ†æï¼Œæˆ‘çœ‹åˆ°äº†ä»¥ä¸‹å†…å®¹ï¼š%1").arg(result);
            }
            
            displayResult("ğŸ‘ï¸ Vision Analysis", currentSpeechText);
            visionCompleted = true;
            speakText(currentSpeechText);
        },
        [this]() { 
            // ç¡®ä¿è§†è§‰å¤„ç†å®Œæˆã€è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && visionCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));
    
    // Task 8: Summary
    taskQueue.enqueue(PresentationTask(
        "summary",
        "Presentation Summary",
        [this]() {
            qInfo() << "\nğŸ¯ Executing Task: Presentation Summary";
            qInfo() << QString(40, '-');
            speechCompleted = false;
            isAudioPlaying = false;
            isSynthesisCompleted = false;
            QString summaryText = "ä»Šå¤©çš„æ¼”ç¤ºåˆ°æ­¤ç»“æŸã€‚"
                                 "æˆ‘ä¸ºå¤§å®¶å±•ç¤ºäº†deepinç³»ç»ŸAIåŠ©æ‰‹çš„å››å¤§æ ¸å¿ƒåŠŸèƒ½ï¼š\n"
                                 "ç¬¬ä¸€ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†åŠŸèƒ½ï¼Œæˆ‘å¯ä»¥ç†è§£å¹¶å›ç­”å„ç§å…³äºdeepinç³»ç»Ÿçš„é—®é¢˜ã€‚"
                                 "ç¬¬äºŒï¼Œè¯­éŸ³è¯†åˆ«åŠŸèƒ½ï¼Œæˆ‘èƒ½å¤Ÿå‡†ç¡®åœ°å°†æ‚¨çš„è¯­éŸ³è½¬æ¢æˆæ–‡å­—ã€‚"
                                 "ç¬¬ä¸‰ï¼Œå›¾åƒè¯†åˆ«åŠŸèƒ½ï¼Œæˆ‘å¯ä»¥åˆ†æå›¾ç‰‡å†…å®¹å¹¶æä¾›è¯¦ç»†æè¿°ã€‚"
                                 "ç¬¬å››ï¼Œè¯­éŸ³åˆæˆåŠŸèƒ½ï¼Œæˆ‘èƒ½å¤Ÿå°†æ–‡å­—è½¬æ¢æˆè‡ªç„¶çš„è¯­éŸ³ã€‚"
                                 "ä½œä¸ºdeepinç³»ç»Ÿçš„AIç”µè„‘ç®¡å®¶ï¼Œ"
                                 "æˆ‘å°†ç«­è¯šä¸ºç”¨æˆ·æä¾›æ™ºèƒ½ã€ä¾¿æ·çš„ç³»ç»Ÿç®¡ç†å’ŒæŠ€æœ¯æ”¯æŒæœåŠ¡ã€‚"
                                 "è°¢è°¢å¤§å®¶è§‚çœ‹ä»Šå¤©çš„æ¼”ç¤ºï¼";
            displayStatus("ğŸ“ Step 5", "ğŸ”Š Speaking", "Presentation Summary");
            speakText(summaryText);
        },
        [this]() { 
            // ç¡®ä¿è¯­éŸ³åˆæˆå®Œæˆä¸”éŸ³é¢‘æ’­æ”¾ä¹Ÿå®Œæˆ
            return speechCompleted && !isAudioPlaying && !isWaitingForSpeech; 
        }
    ));
    
    qInfo() << "âœ… Task queue initialized with" << taskQueue.size() << "tasks";
}

void PresentationDemo::executeNextTask()
{
    if (!isPresentationRunning) return;
    
    if (taskQueue.isEmpty()) {
        qInfo() << "ğŸ‰ All tasks completed! Presentation finished successfully!";
        qInfo() << "ğŸ‘‹ Thank you for watching the Deepin AI Assistant demo!";
        isPresentationRunning = false;
        taskCompletionTimer->stop();
        QTimer::singleShot(200, qApp, &QCoreApplication::quit);
        return;
    }
    
    if (isTaskRunning) {
        qWarning() << "âš ï¸  Task already running, skipping execution";
        return;
    }
    
    // Get next task from queue
    PresentationTask task = taskQueue.dequeue();
    currentTask = new PresentationTask(task);
    isTaskRunning = true;
    
    qInfo() << "ğŸš€ Starting task:" << currentTask->taskId << "-" << currentTask->description;
    
    // Execute the task
    currentTask->executeFunction();
    
    // Start monitoring task completion
    taskCompletionTimer->start();
}

void PresentationDemo::checkTaskCompletion()
{
    if (!isTaskRunning || !currentTask) return;
    
    // Check if current task is completed
    if (currentTask->completionChecker()) {
        completeCurrentTask();
    }
}

void PresentationDemo::completeCurrentTask()
{
    if (!currentTask) return;
    
    qInfo() << "âœ… Task completed:" << currentTask->taskId << "-" << currentTask->description;
    
    // Clean up current task
    delete currentTask;
    currentTask = nullptr;
    isTaskRunning = false;
    taskCompletionTimer->stop();
    
    // Wait a bit before starting next task
    QTimer::singleShot(STEP_DELAY_MS, this, &PresentationDemo::executeNextTask);
}

void PresentationDemo::step1_Welcome()
{
    QString welcomeText = "å¤§å®¶å¥½ï¼æˆ‘æ˜¯deepinç³»ç»Ÿçš„AIç”µè„‘ç®¡å®¶ã€‚"
                         "æˆ‘å¯ä»¥å¸®åŠ©ç”¨æˆ·è§£å†³å„ç§ç³»ç»Ÿé—®é¢˜ï¼Œ"
                         "åŒ…æ‹¬ç³»ç»Ÿè®¾ç½®ã€æ•…éšœè¯Šæ–­ã€ä½¿ç”¨æŒ‡å¯¼ç­‰ã€‚"
                         "ä»Šå¤©æˆ‘å°†ä¸ºå¤§å®¶æ¼”ç¤ºæˆ‘çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œ"
                         "åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œå›¾åƒè¯†åˆ«èƒ½åŠ›ã€‚"
                         "è®©æˆ‘ä»¬å¼€å§‹è¿™æ¬¡æ¼”ç¤ºå§ï¼";

    displayStatus("ğŸ‘‹ Step 1", "ğŸ”Š Speaking", "Welcome and Introduction");
    speakText(welcomeText);
}

// step2_NLPDemo is now integrated into the task queue system

// step3_SpeechDemo, step4_VisionDemo, and step5_Summary are now integrated into the task queue system

void PresentationDemo::speakText(const QString &text)
{
    if (!tts || text.isEmpty()) return;

    isWaitingForSpeech = true;
    isSynthesisCompleted = false;  // é‡ç½®è¯­éŸ³åˆæˆå®Œæˆæ ‡å¿—
    currentSpeechText = text;
    
    // Clear previous audio data
    accumulatedAudioData.clear();
    outputFilename.clear();

    displayStatus("ğŸ”Š Text-to-Speech", "ğŸ”„ Synthesizing",
                  QString("Text length: %1 characters").arg(text.length()));

    qInfo() << "ğŸ¤ Starting stream synthesis for:" << text.left(50) + "...";
    
    bool result = tts->startStreamSynthesis(text);
    if (!result) {
        auto error = tts->lastError();
        qCritical() << "âŒ Failed to start TTS synthesis:" << error.getErrorCode() << error.getErrorMessage();
        
        // Fallback: mark as completed without audio
        speechCompleted = true;
        isWaitingForSpeech = false;
    } else {
        qInfo() << "âœ… TTS synthesis started successfully";
    }
}

void PresentationDemo::setupAudioInput()
{
    if (audioInput) {
        audioInput->stop();
        delete audioInput;
        audioInput = nullptr;
    }

    QAudioDeviceInfo selectedDevice = QAudioDeviceInfo::defaultInputDevice();

    if (!selectedDevice.isFormatSupported(audioFormat)) {
        qWarning() << "âš ï¸  Audio format not supported, using nearest format...";
        audioFormat = selectedDevice.nearestFormat(audioFormat);
    }

    audioInput = new QAudioInput(selectedDevice, audioFormat, this);
    audioInput->setVolume(0.7);

    qInfo() << "âœ… Audio input setup completed";
}

void PresentationDemo::setupAudioOutput()
{
    if (audioOutput) {
        audioOutput->stop();
        delete audioOutput;
        audioOutput = nullptr;
    }

    if (audioBuffer) {
        delete audioBuffer;
        audioBuffer = nullptr;
    }

    QAudioDeviceInfo selectedDevice = QAudioDeviceInfo::defaultOutputDevice();
    audioOutput = new QAudioOutput(selectedDevice, audioFormat, this);
    audioOutput->setVolume(0.8);

    audioBuffer = new QBuffer(this);
    audioBuffer->open(QIODevice::ReadWrite);

    connect(audioOutput, &QAudioOutput::stateChanged,
            this, &PresentationDemo::onAudioStateChanged);

    qInfo() << "âœ… Audio output setup completed";
}

void PresentationDemo::startRecording()
{
    if (!stt) return;

    displayStatus("ğŸ¤ Speech Recognition", "ğŸ”„ Starting", "Listening for speech...");

    currentUserInput.clear();

    QVariantHash params;
    params["language"] = "zh-cn";
    params["format"] = "pcm";
    params["sampleRate"] = audioFormat.sampleRate();
    params["channels"] = audioFormat.channelCount();
    params["bitsPerSample"] = audioFormat.sampleSize();
    params["vad_eos"] = 2000; // 2 seconds of silence to end recognition

    stt->startStreamRecognition(params);
    isRecording = true;

    // Create audio device
    audioDevice = audioInput->start();
    if (audioDevice) {
        connect(audioDevice, &QIODevice::readyRead,
                this, &PresentationDemo::onAudioDataReady);
    }

    recognitionTimeoutTimer->start(RECOGNITION_TIMEOUT_MS);
}

void PresentationDemo::stopRecording()
{
    if (!stt || !isRecording) return;

    isRecording = false;
    recognitionTimeoutTimer->stop();

    displayStatus("ğŸ¤ Speech Recognition", "â¹ï¸  Stopping", "Processing final audio");

    QString finalText = stt->endStreamRecognition();
    if (!finalText.isEmpty()) {
        displayResult("ğŸ“ Speech-to-Text", finalText);
        currentUserInput = finalText;
    }

    if (audioInput) {
        audioInput->stop();
    }
}

void PresentationDemo::onAudioDataReady()
{
    if (!isRecording || !audioDevice) return;

    QByteArray audioData = audioDevice->readAll();
    if (!audioData.isEmpty()) {
        stt->sendAudioData(audioData);
    }
}

void PresentationDemo::onRecognitionPartialResult(const QString &partialText)
{
    if (!partialText.isEmpty()) {
        displayStatus("ğŸ“ Speech-to-Text", "ğŸ”„ Processing", partialText);
        currentUserInput = partialText;
    }
}

void PresentationDemo::onRecognitionResult(const QString &text)
{
    if (!text.isEmpty()) {
        displayResult("ğŸ“ Speech-to-Text", text);
        currentUserInput = text;
        stopRecording();
        recognitionCompleted = true;
    }
}

void PresentationDemo::onRecognitionError(int errorCode, const QString &errorMessage)
{
    displayStatus("ğŸ¤ Speech Recognition", "âŒ Error",
                  QString("Code: %1, Message: %2").arg(errorCode).arg(errorMessage));
    stopRecording();
    recognitionCompleted = true; // Mark as completed even on error
}

void PresentationDemo::onRecognitionCompleted(const QString &finalText)
{
    displayStatus("ğŸ¤ Speech Recognition", "âœ… Completed", finalText);
    if (!finalText.isEmpty()) {
        currentUserInput = finalText;
    }
    recognitionCompleted = true;
}

void PresentationDemo::onStreamOutput(const QString &content)
{
    displayStatus("ğŸ¤– AI Chat", "ğŸ”„ Streaming", content);
}

void PresentationDemo::onStreamFinished(int error)
{
    if (error == 0) {
        chatCompleted = true;
    } else {
        displayStatus("ğŸ¤– AI Chat", "âŒ Error", QString("Error code: %1").arg(error));
        chatCompleted = true; // Mark as completed even on error
    }
}

void PresentationDemo::onSynthesisResult(const QByteArray &audioData)
{
    if (!audioData.isEmpty()) {
        accumulatedAudioData.append(audioData);
//        qInfo() << "ğŸ“¡ Received audio chunk:" << audioData.size() << "bytes, total:" << accumulatedAudioData.size() << "bytes";
    }
}

void PresentationDemo::onSynthesisError(int errorCode, const QString &errorMessage)
{
    displayStatus("ğŸ”Š Text-to-Speech", "âŒ Error",
                  QString("Code: %1, Message: %2").arg(errorCode).arg(errorMessage));
    isWaitingForSpeech = false;
    speechCompleted = true; // Mark as completed even on error
}

void PresentationDemo::onSynthesisCompleted(const QByteArray &/*finalAudio*/)
{
    displayStatus("ğŸ”Š Text-to-Speech", "âœ… Synthesis Completed", "Processing audio data...");

    isSynthesisCompleted = true;  // æ ‡è®°è¯­éŸ³åˆæˆå·²å®Œæˆ
    qInfo() << "ğŸ¤ Speech synthesis completed, total audio data:" << accumulatedAudioData.size() << "bytes";

    if (!accumulatedAudioData.isEmpty()) {
        qInfo() << "ğŸµ Total audio data received:" << accumulatedAudioData.size() << "bytes";
        
        // Save complete audio to file
        saveAudioToFile(accumulatedAudioData, outputFilename);
        
        // Play the complete audio file
        if (!outputFilename.isEmpty()) {
            playAudioFile(outputFilename);
        } else {
            speechCompleted = true;
            isWaitingForSpeech = false;
        }
    } else {
        qWarning() << "âš ï¸  No audio data received from synthesis";
        speechCompleted = true;
        isWaitingForSpeech = false;
    }
}

void PresentationDemo::playAudioFile(const QString &filename)
{
    qDebug() << "--------------------------------------------\n";
    if (!audioOutput || !audioBuffer) {
        speechCompleted = true;
        isWaitingForSpeech = false;
        return;
    }

    QFile file(filename);
    if (!file.open(QIODevice::ReadOnly)) {
        qWarning() << "âš ï¸  Failed to open audio file:" << filename;
        speechCompleted = true;
        isWaitingForSpeech = false;
        return;
    }

    QByteArray audioData = file.readAll();
    file.close();

    if (audioData.isEmpty()) {
        qWarning() << "âš ï¸  Audio file is empty:" << filename;
        speechCompleted = true;
        isWaitingForSpeech = false;
        return;
    }

    // æ­£ç¡®çš„æ–¹å¼ï¼šå®Œå…¨æ›¿æ¢ç¼“å†²åŒºæ•°æ®ï¼Œé¿å…æ•°æ®æ±¡æŸ“
    audioBuffer->close();
    audioBuffer->setData(audioData);
    audioBuffer->open(QIODevice::ReadOnly);

    isAudioPlaying = true;  // æ ‡è®°éŸ³é¢‘å¼€å§‹æ’­æ”¾
    audioOutput->start(audioBuffer);

    displayStatus("ğŸµ Audio Playback", "ğŸ”Š Playing",
                  QString("Size: %1 bytes").arg(audioData.size()));
}

void PresentationDemo::onAudioStateChanged(QAudio::State state)
{
    qDebug() << "+++++++++++++++++++++++++++++++++++++\n" << state;
    switch (state) {
    case QAudio::ActiveState:
        displayStatus("ğŸµ Audio Playback", "ğŸ”Š Active", "Playing audio");
        // å¦‚æœéŸ³é¢‘é‡æ–°å¼€å§‹æ’­æ”¾ï¼Œå–æ¶ˆå»¶è¿Ÿç¡®è®¤å®šæ—¶å™¨
        if (idleConfirmationTimer->isActive()) {
            idleConfirmationTimer->stop();
            qInfo() << "ğŸ¤ Audio resumed, cancelling idle confirmation timer";
        }
        break;
    case QAudio::SuspendedState:
        displayStatus("ğŸµ Audio Playback", "â¸ï¸  Suspended", "Audio paused");
        break;
    case QAudio::StoppedState:
        displayStatus("ğŸµ Audio Playback", "â¹ï¸  Stopped", "Audio finished");
//        speechCompleted = true;
        isWaitingForSpeech = false;
        isAudioPlaying = false;  // æ¸…é™¤éŸ³é¢‘æ’­æ”¾æ ‡å¿—
        break;
    case QAudio::IdleState:
        displayStatus("ğŸµ Audio Playback", "ğŸ’¤ Idle", "Audio playback idle");
        isAudioPlaying = false;  // æ¸…é™¤éŸ³é¢‘æ’­æ”¾æ ‡å¿—
        
        // å¯åŠ¨å»¶è¿Ÿç¡®è®¤å®šæ—¶å™¨ï¼Œé¿å…å› æ’­æ”¾é€Ÿåº¦å¿«äºåˆæˆé€Ÿåº¦è€Œè¯¯åˆ¤å®Œæˆ
        if (isSynthesisCompleted) {
            qInfo() << "ğŸ¤ Audio idle and synthesis completed, confirming completion in" << IDLE_CONFIRMATION_MS << "ms";
            idleConfirmationTimer->start(IDLE_CONFIRMATION_MS);
        } else {
            qInfo() << "ğŸ¤ Audio idle but synthesis not completed, waiting for more audio data...";
        }
        break;
    case QAudio::InterruptedState:
        displayStatus("ğŸµ Audio Playback", "âš ï¸  Interrupted", "Audio interrupted");
        speechCompleted = true;
        isWaitingForSpeech = false;
        isAudioPlaying = false;  // æ¸…é™¤éŸ³é¢‘æ’­æ”¾æ ‡å¿—
        break;
    }
}

void PresentationDemo::saveAudioToFile(const QByteArray &audioData, const QString &/*filename*/)
{
    QString timestamp = QDateTime::currentDateTime().toString("yyyyMMdd_hhmmss");
    outputFilename = QString("/tmp/presentation_audio_%1.wav").arg(timestamp);

    // Create WAV header for PCM format
    QByteArray wavHeader;
    wavHeader.resize(44);

    // RIFF header
    wavHeader.replace(0, 4, "RIFF");
    qint32 fileSize = audioData.size() + 36;
    wavHeader.replace(4, 4, QByteArray::fromRawData(reinterpret_cast<const char*>(&fileSize), 4));
    wavHeader.replace(8, 4, "WAVE");

    // fmt chunk
    wavHeader.replace(12, 4, "fmt ");
    qint32 fmtSize = 16;
    wavHeader.replace(16, 4, QByteArray::fromRawData(reinterpret_cast<const char*>(&fmtSize), 4));
    qint16 audioFormat = 1; // PCM
    wavHeader.replace(20, 2, QByteArray::fromRawData(reinterpret_cast<const char*>(&audioFormat), 2));
    qint16 numChannels = 1; // Mono
    wavHeader.replace(22, 2, QByteArray::fromRawData(reinterpret_cast<const char*>(&numChannels), 2));
    qint32 sampleRate = 16000;
    wavHeader.replace(24, 4, QByteArray::fromRawData(reinterpret_cast<const char*>(&sampleRate), 4));
    qint32 byteRate = sampleRate * numChannels * 2; // 16-bit
    wavHeader.replace(28, 4, QByteArray::fromRawData(reinterpret_cast<const char*>(&byteRate), 4));
    qint16 blockAlign = numChannels * 2;
    wavHeader.replace(32, 2, QByteArray::fromRawData(reinterpret_cast<const char*>(&blockAlign), 2));
    qint16 bitsPerSample = 16;
    wavHeader.replace(34, 2, QByteArray::fromRawData(reinterpret_cast<const char*>(&bitsPerSample), 2));

    // data chunk
    wavHeader.replace(36, 4, "data");
    qint32 dataSize = audioData.size();
    wavHeader.replace(40, 4, QByteArray::fromRawData(reinterpret_cast<const char*>(&dataSize), 4));

    // Write to file
    QFile file(outputFilename);
    if (file.open(QIODevice::WriteOnly)) {
        file.write(wavHeader);
        file.write(audioData);
        file.close();
        qInfo() << "ğŸ’¾ Audio saved to:" << outputFilename;
    } else {
        qWarning() << "âš ï¸  Failed to save audio file:" << outputFilename;
    }
}

void PresentationDemo::displayStatus(const QString &module, const QString &status, const QString &details)
{
    QString timestamp = QDateTime::currentDateTime().toString("HH:mm:ss");
    qInfo() << QString("[%1] %2 %3").arg(timestamp, module, status);
    if (!details.isEmpty()) {
        qInfo() << "    â””â”€" << details;
    }
}

void PresentationDemo::displayResult(const QString &module, const QString &result)
{
    QString timestamp = QDateTime::currentDateTime().toString("HH:mm:ss");
    qInfo() << QString("[%1] %2 âœ… Result:").arg(timestamp, module);
    qInfo() << "    â””â”€" << QString("\"%1\"").arg(result);
}

int main(int argc, char *argv[])
{
    QCoreApplication app(argc, argv);

    // Set application information
    app.setApplicationName("Deepin AI Presentation Demo");
    app.setApplicationVersion("1.0.0");
    app.setOrganizationName("UnionTech Software Technology Co., Ltd.");

    qInfo() << "Deepin AI Presentation Demo - Version 1.0.0";
    qInfo() << "Comprehensive demonstration of AI capabilities";
    qInfo() << "Organization: UnionTech Software Technology Co., Ltd.";
    qInfo() << "";

    PresentationDemo demo;
    demo.startPresentation();

    return app.exec();
}

#include "presentation_demo.moc"
